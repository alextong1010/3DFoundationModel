# foundation model
vision_encoder: clip # resnet, clip or dinov2
backbone: 'ViT-g-14'
weights: laion2b_s34b_b88k

abs_action: true
shape_meta: 
  action:
    shape:
    - 10
  obs:
    agentview_image:
      shape:
      - 3
      - 84
      - 84
      type: rgb
    robot0_eef_pos:
      shape:
      - 3
    robot0_eef_quat:
      shape:
      - 4
    robot0_eye_in_hand_image:
      shape:
      - 3
      - 84
      - 84
      type: rgb
    robot0_gripper_qpos:
      shape:
      - 2

# dirs
display_name: manipulation_eval
dataset_path: /n/netscratch/hankyang_lab/Lab/alex/robomimic/datasets/can/mh/image_abs.hdf5

# hyperparams
num_epochs: 500
num_diffusion_iters: 100
num_demos: 200
num_warmup_steps: 500
pred_horizon: 16
obs_horizon: 2
action_horizon: 8
max_steps: 300
eval_epoch: 20
lr: 0.0001
weight_decay: 0.000001
batch_size: 64
wandb: true
verbose: false
ratio: 0.9 # percent of dataset that is split into training data
